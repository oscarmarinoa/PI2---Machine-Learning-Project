{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this is a different file we are going to import the needed files and libraries.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(r'C:\\Users\\Oscar Marino\\Documents\\Henry\\DFT 04 - Henry Course\\Proyectos Individuales\\PI2---Machine-Learning-Project\\Train_pipeline')\n",
    "data_standardized = pd.read_csv(r'C:\\Users\\Oscar Marino\\Documents\\Henry\\DFT 04 - Henry Course\\Proyectos Individuales\\PI2---Machine-Learning-Project\\data_standardized_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92354, 8) (39581, 8) (92354,) (39581,)\n"
     ]
    }
   ],
   "source": [
    "# To define the model we are going to use the data_standarized Dataframe that is the Train dataset that is standardized.\n",
    "X = data_standardized\n",
    "y = train['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# We are going to split our data in Train and Test to assess our model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify = y)\n",
    "\n",
    "# The train data will be our dev for experimenting with different models and hyperparameters, and the test set will se our hold-out.\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines are goint to be created to automate the workflow for our ML model. \n",
    "# We are going to different pipelines to test our data.\n",
    "\n",
    "# Pipeline with Decision Trees \n",
    "pipe_dt = Pipeline([('scl', StandardScaler()), ('pca', PCA(n_components=2)), ('clf', DecisionTreeClassifier(max_depth=11,random_state=42))])\n",
    "\n",
    "# Pipeline with K Neirest Neighbors\n",
    "pipe_knn = Pipeline([('scl', StandardScaler()), ('pca', PCA(n_components=2)), ('clf', KNeighborsClassifier(n_neighbors=5))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we save them in a list to iterate over them.\n",
    "pipelines = [pipe_dt, pipe_knn]\n",
    "\n",
    "# We create a dictionary with the names and order of the model.\n",
    "pipe_dict = {0: 'Decision Trees', 1: 'K Neirest Neighbors'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the models.\n",
    "for model in pipelines:\n",
    "\tmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Trees pipeline accuracy in train: 0.903\n",
      "K Neirest Neighbors pipeline accuracy in train: 0.918\n"
     ]
    }
   ],
   "source": [
    "# We assess the performance of our model according to the accuracy and reccal metrics.\n",
    "for index, model in enumerate(pipelines):\n",
    "\tprint('%s pipeline accuracy in train: %.3f' % (pipe_dict[index], model.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with best accuracy K Neirest Neighbors\n"
     ]
    }
   ],
   "source": [
    "# Identifying the best model for the test data.T\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_pipeline = ''\n",
    "for index, model in enumerate(pipelines):\n",
    "\tif model.score(X_test, y_test) > best_acc:\n",
    "\t\tbest_acc = model.score(X_test, y_test)\n",
    "\t\tbest_pipe = model\n",
    "\t\tbest_clf = index\n",
    "print('Model with best accuracy %s' % pipe_dict[best_clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_pipeline.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We save the pipeline using Joblib\n",
    "import joblib\n",
    "\n",
    "# dump function allow us to use an arbitrary Python object into one file.\n",
    "joblib.dump(best_pipeline, 'best_pipeline.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For using the pipeline we create an instance of the file saved.\n",
    "best_model = joblib.load('best_pipeline.pkl')\n",
    "\n",
    "# Then we can use it importing the desire data into the following line.\n",
    "best_model.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68ac5f2a079eb37b29a6a78efb89b55e3f5ce774c913a43ce907979c16c14b5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
